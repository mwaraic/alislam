{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qRjVe1tZhsx"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, you must install the packages and set the necessary environment variables.\n",
        "\n",
        "### Installation\n",
        "\n",
        "Install LangChain's Python library, `langchain` and LangChain's integration package for Gemini, `langchain-google-genai`. Next, install LangChain's integration package for the new version of Pinecone, `langchain-pinecone` and the `pinecone-client`, which is Pinecone's Python SDK. Finally, install `langchain-community` to access the `WebBaseLoader` module later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olK4Ejjzuj76"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet langchain-core\n",
        "%pip install --quiet langchain\n",
        "%pip install --quiet langchain-google-genai\n",
        "%pip install --quiet -U langchain-community\n",
        "%pip install --quiet pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQOGMejVu-6D"
      },
      "source": [
        "## Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY` and `COHERE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ysayz8skEfBW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "GOOGLE_API_KEY=os.environ.get('GOOGLE_API_KEY')\n",
        "COHERE_API_KEY=os.environ.get('COHERE_API_KEY')\n",
        "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPhs4mDkjdgY"
      },
      "source": [
        "## Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcvGPVdXu05F"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain import PromptTemplate\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "from pinecone import PodSpec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8NXNTrjp0jdh"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-exp-03-07\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup VectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "n1VwhUQMvpcN"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone as pc\n",
        "\n",
        "PINECONE_HOST=\"DENSE_HOST\"\n",
        "PINECONE_INDEX=\"DENSE_INDEX\"\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pine_client = pc(api_key=PINECONE_API_KEY)\n",
        "index_name = PINECONE_INDEX\n",
        "index = pine_client.Index(index_name, PINECONE_HOST)\n",
        "\n",
        "vectorstore = PineconeVectorStore(\n",
        "    namespace=\"NAMESPACE\",\n",
        "    index=index,\n",
        "    embedding=gemini_embeddings\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone as pc\n",
        "\n",
        "PINECONE_SPARSE_HOST=\"SPARSE_HOST\"\n",
        "PINECONE_SPARSE_INDEX=\"SPARSE_INDEX\"\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pine_sparse_client = pc(api_key=PINECONE_API_KEY)\n",
        "sparse_index_name = PINECONE_SPARSE_INDEX\n",
        "sparse_index = pine_sparse_client.Index(sparse_index_name, PINECONE_SPARSE_HOST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "reranker = CohereRerank(\n",
        "    cohere_api_key=COHERE_API_KEY,\n",
        "    model=\"rerank-multilingual-v3.0\"  # latest model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# To configure model parameters use the `generation_config` parameter.\n",
        "# eg. generation_config = {\"temperature\": 0.7, \"topP\": 0.8, \"topK\": 40}\n",
        "# If you only want to set a custom temperature for the model use the\n",
        "# \"temperature\" parameter directly.\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-05-20\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hybrid Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add dense documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "books = {\n",
        "    \"quran-37\": {\n",
        "        \"name\": \"The Holy Quran with Five Volume Commentary (Vol 1)\",\n",
        "        \"start_page\": 377,\n",
        "        \"end_page\": 816\n",
        "    }\n",
        "}\n",
        "\n",
        "documents = []\n",
        "\n",
        "for book_id, book_data in books.items():\n",
        "    for i in range(book_data[\"start_page\"], book_data[\"end_page\"] + 1):\n",
        "        url = f\"https://new.alislam.org/api/books/text?id={book_id}&pages={i}\"\n",
        "        response = requests.get(url)\n",
        "        entry = response.json()[0]\n",
        "        text = entry['content']\n",
        "        page_number = entry.get('printPageNum', '')\n",
        "        documents.extend([Document(page_content=text, metadata={\"chunk_index\": i, \"volume\": \"1\",\"title\": book_data[\"name\"], \"page\": page_number, \"link\": f\"https://new.alislam.org/library/books/quran-english-five-volume-1?option=options&page={entry.get('pageNum', '')}\"})])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from random import uniform\n",
        "from time import sleep\n",
        "\n",
        "def add_with_backoff(documents_batch, max_retries=10):\n",
        "    retry = 0\n",
        "    while retry <= max_retries:\n",
        "        try:\n",
        "            vectorstore.add_documents(documents_batch)\n",
        "            return  # Success: exit function\n",
        "        except Exception as e:\n",
        "            if '429' in str(e):  # or check e.status_code == 429 if it's a ResponseError\n",
        "                wait = 2 ** retry + uniform(0, 1)  # jitter to avoid thundering herd\n",
        "                print(f\"429 Too Many Requests. Retrying in {wait:.2f} seconds...\")\n",
        "                time.sleep(wait)\n",
        "                retry += 1\n",
        "            else:\n",
        "                raise  # re-raise other exceptions\n",
        "\n",
        "# Batch processing loop with delay and backoff\n",
        "for i in range(0, len(documents), 50):\n",
        "    batch = documents[i:i+50]\n",
        "    add_with_backoff(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add sparse documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2024 The GTE Team Authors and Alibaba Group.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from transformers.utils import is_torch_npu_available\n",
        "\n",
        "\n",
        "class GTEEmbeddidng(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 normalized: bool = True,\n",
        "                 use_fp16: bool = True,\n",
        "                 device: str = None\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.normalized = normalized\n",
        "        if device:\n",
        "            self.device = torch.device(device)\n",
        "        else:\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device(\"cuda\")\n",
        "            elif torch.backends.mps.is_available():\n",
        "                self.device = torch.device(\"mps\")\n",
        "            elif is_torch_npu_available():\n",
        "                self.device = torch.device(\"npu\")\n",
        "            else:\n",
        "                self.device = torch.device(\"cpu\")\n",
        "                use_fp16 = False\n",
        "        self.use_fp16 = use_fp16\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
        "            model_name, trust_remote_code=True, torch_dtype=torch.float16 if self.use_fp16 else None\n",
        "        )\n",
        "        self.vocab_size = self.model.config.vocab_size\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def _process_token_weights(self, token_weights: np.ndarray, input_ids: list):\n",
        "        # conver to dict\n",
        "        result = defaultdict(int)\n",
        "        unused_tokens = set([self.tokenizer.cls_token_id, self.tokenizer.eos_token_id, self.tokenizer.pad_token_id,\n",
        "                             self.tokenizer.unk_token_id])\n",
        "        # token_weights = np.ceil(token_weights * 100)\n",
        "        for w, idx in zip(token_weights, input_ids):\n",
        "            if idx not in unused_tokens and w > 0:\n",
        "                token = self.tokenizer.decode([int(idx)])\n",
        "                if w > result[token]:\n",
        "                    result[token] = w\n",
        "        return result\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self,\n",
        "               texts: None,\n",
        "               dimension: int = None,\n",
        "               max_length: int = 8192,\n",
        "               batch_size: int = 16,\n",
        "               return_dense: bool = True,\n",
        "               return_sparse: bool = False):\n",
        "        if dimension is None:\n",
        "            dimension = self.model.config.hidden_size\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        num_texts = len(texts)\n",
        "        all_dense_vecs = []\n",
        "        all_token_weights = []\n",
        "        for n, i in enumerate(range(0, num_texts, batch_size)):\n",
        "            batch = texts[i: i + batch_size]\n",
        "            resulst = self._encode(batch, dimension, max_length, batch_size, return_dense, return_sparse)\n",
        "            if return_dense:\n",
        "                all_dense_vecs.append(resulst['dense_embeddings'])\n",
        "            if return_sparse:\n",
        "                all_token_weights.extend(resulst['token_weights'])\n",
        "        all_dense_vecs = torch.cat(all_dense_vecs, dim=0)\n",
        "        return {\n",
        "            \"dense_embeddings\": all_dense_vecs,\n",
        "            \"token_weights\": all_token_weights \n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _encode(self,\n",
        "                texts: Dict[str, torch.Tensor] = None,\n",
        "                dimension: int = None,\n",
        "                max_length: int = 1024,\n",
        "                batch_size: int = 16,\n",
        "                return_dense: bool = True,\n",
        "                return_sparse: bool = False):\n",
        "\n",
        "        text_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
        "        text_input = {k: v.to(self.model.device) for k,v in text_input.items()}\n",
        "        model_out = self.model(**text_input, return_dict=True)\n",
        "\n",
        "        output = {}\n",
        "        if return_dense:\n",
        "            dense_vecs = model_out.last_hidden_state[:, 0, :dimension]\n",
        "            if self.normalized:\n",
        "                dense_vecs = torch.nn.functional.normalize(dense_vecs, dim=-1)\n",
        "            output['dense_embeddings'] = dense_vecs\n",
        "        if return_sparse:\n",
        "            token_weights = torch.relu(model_out.logits).squeeze(-1)\n",
        "            token_weights = list(map(self._process_token_weights, token_weights.detach().cpu().numpy().tolist(),\n",
        "                                                    text_input['input_ids'].cpu().numpy().tolist()))\n",
        "            output['token_weights'] = token_weights\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _compute_sparse_scores(self, embs1, embs2):\n",
        "        scores = 0\n",
        "        for token, weight in embs1.items():\n",
        "            if token in embs2:\n",
        "                scores += weight * embs2[token]\n",
        "        return scores\n",
        "\n",
        "    def compute_sparse_scores(self, embs1, embs2):\n",
        "        scores = [self._compute_sparse_scores(emb1, emb2) for emb1, emb2 in zip(embs1, embs2)]\n",
        "        return np.array(scores)\n",
        "\n",
        "    def compute_dense_scores(self, embs1, embs2):\n",
        "        scores = torch.sum(embs1*embs2, dim=-1).cpu().detach().numpy()\n",
        "        return scores\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_scores(self, \n",
        "        text_pairs: List[Tuple[str, str]], \n",
        "        dimension: int = None,\n",
        "        max_length: int = 1024,\n",
        "        batch_size: int = 16,\n",
        "        dense_weight=1.0,\n",
        "        sparse_weight=0.1):\n",
        "        text1_list = [text_pair[0] for text_pair in text_pairs]\n",
        "        text2_list = [text_pair[1] for text_pair in text_pairs]\n",
        "        embs1 = self.encode(text1_list, dimension, max_length, batch_size, return_dense=True, return_sparse=True)\n",
        "        embs2 = self.encode(text2_list, dimension, max_length, batch_size, return_dense=True, return_sparse=True)\n",
        "        scores = self.compute_dense_scores(embs1['dense_embeddings'], embs2['dense_embeddings']) * dense_weight + \\\n",
        "            self.compute_sparse_scores(embs1['token_weights'], embs2['token_weights']) * sparse_weight\n",
        "        scores = scores.tolist()\n",
        "        return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove empty documents\n",
        "\n",
        "updated_documents = []\n",
        "for document in documents:\n",
        "    if document.page_content != \"\":\n",
        "        updated_documents.append(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from time import sleep\n",
        "\n",
        "model_name_or_path = 'Alibaba-NLP/gte-multilingual-base'\n",
        "model = GTEEmbeddidng(model_name_or_path)\n",
        "\n",
        "BATCH_SIZE = 25\n",
        "\n",
        "# Helper to chunk data\n",
        "def chunked(iterable, size):\n",
        "    for i in range(0, len(iterable), size):\n",
        "        yield iterable[i:i + size]\n",
        "\n",
        "for batch_num, doc_batch in enumerate(chunked(updated_documents, BATCH_SIZE), start=1):\n",
        "    # Prepare records\n",
        "    records = [\n",
        "        {\n",
        "            \"id\": str(random.randint(1, 1000000)),\n",
        "            \"text\": document.page_content,\n",
        "            **document.metadata\n",
        "        }\n",
        "        for document in doc_batch\n",
        "    ]\n",
        "\n",
        "    # Get documents and embed\n",
        "    docs = [r[\"text\"] for r in records]\n",
        "    embs = model.encode(docs, return_sparse=True)\n",
        "\n",
        "    # Prepare vectors for upsert\n",
        "    vectors = []\n",
        "    for i, record in enumerate(records):\n",
        "        token_weights = embs[\"token_weights\"][i]\n",
        "        merged = {}\n",
        "\n",
        "        for token, weight in token_weights.items():\n",
        "            token_id = model.tokenizer.convert_tokens_to_ids(token)\n",
        "            merged[token_id] = merged.get(token_id, 0.0) + float(weight)\n",
        "\n",
        "        indices = list(merged.keys())\n",
        "        values = list(merged.values())\n",
        "\n",
        "        vectors.append({\n",
        "            \"id\": record[\"id\"],\n",
        "            \"sparse_values\": {\n",
        "                \"indices\": indices,\n",
        "                \"values\": values\n",
        "            },\n",
        "            \"metadata\": {\n",
        "                **record,\n",
        "                \"text\": record[\"text\"]\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Upsert batch\n",
        "    sparse_index.upsert(vectors=vectors, namespace=\"NAMESPACE\")\n",
        "    print(f\"✅ Upserted batch {batch_num}\")\n",
        "    sleep(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hybrid Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Commentary Search Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_commentary(query):\n",
        "    \"\"\"\n",
        "    Search the commentary for the given query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query to search the commentary for.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of matches from the commentary.\n",
        "    \"\"\"\n",
        "    dense_results = index.query(\n",
        "        namespace=\"NAMESPACE\",\n",
        "        vector=gemini_embeddings.embed_query(query), \n",
        "        top_k=40,\n",
        "        include_metadata=True,\n",
        "        include_values=False\n",
        "    )\n",
        "\n",
        "    # Encode the query and extract sparse token weights\n",
        "\n",
        "    q_tw = model.encode([query], return_sparse=True)[\"token_weights\"][0]\n",
        "\n",
        "    # Merge duplicate token indices by summing their weights\n",
        "\n",
        "    merged = {}\n",
        "    for token, weight in q_tw.items():\n",
        "        token_id = model.tokenizer.convert_tokens_to_ids(token)\n",
        "        if token_id in merged:\n",
        "            merged[token_id] += float(weight)\n",
        "        else:\n",
        "            merged[token_id] = float(weight)\n",
        "\n",
        "    q_indices = list(merged.keys())\n",
        "    q_values = list(merged.values())\n",
        "\n",
        "    # Perform sparse query\n",
        "\n",
        "    sparse_results = sparse_index.query(\n",
        "        top_k=40,\n",
        "        sparse_vector={\n",
        "            \"indices\": q_indices,\n",
        "            \"values\": q_values\n",
        "        },\n",
        "        namespace=\"NAMESPACE\",\n",
        "        include_metadata=True,\n",
        "    )\n",
        "\n",
        "    def merge_chunks(dense_results, sparse_results):\n",
        "        def normalize_hit(hit, source='dense'):\n",
        "            if source == 'dense':\n",
        "                return {\n",
        "                    'chunk_text': hit['metadata'].get('text', '').strip(),\n",
        "                    '_score': hit.get('score', 0),\n",
        "                    'link': hit['metadata'].get('link'),\n",
        "                    'page': hit['metadata'].get('page'),\n",
        "                    'title': hit['metadata'].get('title'),\n",
        "                    'source': 'dense'\n",
        "                }\n",
        "            elif source == 'sparse':\n",
        "                return {\n",
        "                    'chunk_text': hit['metadata'].get('text', '').strip(),\n",
        "                    '_score': hit.get('score', 0),\n",
        "                    'link': hit['metadata'].get('link'),\n",
        "                    'page': hit['metadata'].get('page'),\n",
        "                    'title': hit['metadata'].get('title'),\n",
        "                    'source': 'sparse'\n",
        "                }\n",
        "\n",
        "        # Normalize both result sets\n",
        "\n",
        "        dense_hits = [normalize_hit(hit, source='dense') for hit in dense_results.get('matches', [])]\n",
        "        sparse_hits = [normalize_hit(hit, source='sparse') for hit in sparse_results.get('matches', [])]\n",
        "        \n",
        "        # Deduplicate based on `chunk_text`\n",
        "\n",
        "        unique_hits = {}\n",
        "        for hit in dense_hits + sparse_hits:\n",
        "            text_key = hit['chunk_text']\n",
        "            if text_key and text_key not in unique_hits:\n",
        "                unique_hits[text_key] = hit\n",
        "            elif text_key and text_key in unique_hits:\n",
        "                # Keep the one with higher score\n",
        "                if hit['_score'] > unique_hits[text_key]['_score']:\n",
        "                    unique_hits[text_key] = hit\n",
        "\n",
        "        # Sort by score\n",
        "\n",
        "        sorted_hits = sorted(unique_hits.values(), key=lambda x: x['_score'], reverse=True)\n",
        "\n",
        "        return sorted_hits\n",
        "\n",
        "    merged_results = merge_chunks(dense_results, sparse_results)\n",
        "\n",
        "    reranked_results = reranker.rerank(merged_results, query, top_n=5)\n",
        "\n",
        "    final_results = []\n",
        "\n",
        "    for result in reranked_results:\n",
        "        final_results.append(merged_results[result['index']])\n",
        "    \n",
        "    # enhance results with context from nearby pages\n",
        "    \n",
        "    enhanced_results = []\n",
        "\n",
        "    for result in final_results:\n",
        "        pages = [str(int(result['page']) - 1), str(int(result['page'])), str(int(result['page']) + 1)]\n",
        "\n",
        "        dummy_vector = [0.0] * 3072  # Adjust to your index's dimensionality\n",
        "\n",
        "        result = index.query(\n",
        "            vector=dummy_vector,\n",
        "            namespace=\"NAMESPACE\",\n",
        "            top_k=3,\n",
        "            filter={\n",
        "                \"page\": { \"$in\": pages }\n",
        "            },\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        enhanced_results.extend(result['matches'])\n",
        "    \n",
        "    return enhanced_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Commentary Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "graph = create_react_agent(\n",
        "    llm,\n",
        "    tools=[search_commentary],\n",
        "    prompt=f\"\"\"You are an Ahmadi scholar who answers questions from the Holy Quran 5 Volume Commentary by Hazrat Musleh Maud R.A.\n",
        "\n",
        "Guidelines:\n",
        "\n",
        "SEARCH AND TRANSLATION:\n",
        "- ALWAYS translate the user's query before searching:\n",
        "  * If query contains English words, translate them to Arabic equivalents\n",
        "  * If query contains Arabic words, also try English equivalents\n",
        "- Use the search_commentary tool with BOTH original query AND translated versions\n",
        "- Try multiple search variations systematically:\n",
        "  1. Search with original query\n",
        "  2. Search with Arabic translation\n",
        "  3. Search with English translation\n",
        "  4. Search with alternative spellings/transliterations\n",
        "\n",
        "REFERENCING AND CITATIONS:\n",
        "- Always add page references in format: (Pg. X) or (Pages X-Y) \n",
        "- Include Quran verse references as clickable links: https://www.alislam.org/quran/app/CHAPTER:VERSE\n",
        "- Example: https://www.alislam.org/quran/app/2:255 for Ayat-ul-Kursi (Chapter 2, Verse 255)\n",
        "- When referencing multiple verses, provide separate links for each\n",
        "\n",
        "TRANSLATION EXAMPLES:\n",
        "- \"prayer\" → search: \"prayer\", \"salah\", \"salat\", \"صلاة\"\n",
        "- \"fasting\" → search: \"fasting\", \"sawm\", \"صوم\"\n",
        "- \"charity\" → search: \"charity\", \"zakat\", \"زكاة\"\n",
        "\n",
        "SEARCH STRATEGY:\n",
        "- Always perform multiple searches with different terms\n",
        "- Don't stop after first search - try at least 2-3 variations\n",
        "- If searching for Arabic concepts, include both Arabic and English terms\n",
        "- If a specific verse number is queried but doesn't exist, respond: \"Verse doesn't exist. Please check the chapter and verse number and try again.\"\n",
        "- If no relevant information is found after searching, respond: \"I didn't find information about this topic in the commentary. Please try rephrasing your question or ask about a different topic.\"\n",
        "- If the query is unclear, ask for clarification before searching\n",
        "\n",
        "ANSWER FORMATTING:\n",
        "- Provide comprehensive answers when information is found\n",
        "- Structure responses with clear explanations\n",
        "- Include both the Arabic term and its meaning when discussing Arabic words\n",
        "- Quote relevant passages from the commentary when applicable\n",
        "- Maintain scholarly tone appropriate for religious discourse\n",
        "\n",
        "QUALITY ASSURANCE:\n",
        "- Verify verse numbers before creating links\n",
        "- Cross-reference information when multiple sources are mentioned\n",
        "- Ensure accuracy of Arabic transliterations\n",
        "- Cross reference the translation with the Quran to ensure accuracy\"\"\"\n",
        ")\n",
        "\n",
        "await graph.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": f\"\"\"inamal amalu biniat what is the meaning of this phrase?\"\"\"}]})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Gemini_LangChain_QA_Pinecone_WebLoad.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
